{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunosampaiio/trabalhoBancoDeDados/blob/main/trabalho_big_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VdeqhNf_WDQ"
      },
      "source": [
        "# Comandos para realiza√ß√£o do trabalho da mat√©ria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "Este notebook foi projetado para guiar os alunos na realiza√ß√£o das pr√°ticas de Big Data utilizando PySpark. Certifique-se de seguir cada etapa cuidadosamente para garantir a correta execu√ß√£o das atividades.\n",
        "\n",
        "Seu trabalho come√ßar√° na c√©lula 5. Execute as 4 primeiras c√©lulas para iniciar a atividade.\n",
        "\n",
        "## <font color=red>Observa√ß√£o importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas n√£o ser√° aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ygjBN7_WDS"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab\n",
        "\n",
        "Aqui, voc√™ far√° o download do dataset necess√°rio para as atividades. Certifique-se de que o arquivo foi descompactado corretamente antes de prosseguir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gSJdhz0S_WDT",
        "outputId": "76939999-b3cf-45af-f301-77dffc7c0fe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-30 16:58:58--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‚Äòimdb-reviews-pt-br.zip‚Äô\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M   106MB/s    in 0.4s    \n",
            "\n",
            "2025-03-30 16:58:59 (106 MB/s) - ‚Äòimdb-reviews-pt-br.zip‚Äô saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHGhEqVq_WDU"
      },
      "source": [
        "## Instala√ß√£o manual das depend√™ncias para uso do pyspark no Google Colab\n",
        "\n",
        "Esta etapa garante que todas as bibliotecas necess√°rias para o PySpark sejam instaladas no Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5JADgNvs_WDU",
        "outputId": "f73f799e-0a85-4e9e-97f1-0af5b1dccc30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss8VbYV0_WDU"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession\n",
        "\n",
        "A SparkSession √© o ponto de entrada para usar o PySpark. Certifique-se de configurar corretamente o nome do aplicativo e o master."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "cmiRoDhn_WDV",
        "outputId": "fd6c9fe1-5c49-47c9-edc3-6765e7419064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando spark session para Bruno - RU 3666900\n",
            "Sess√£o iniciado com sucesso! üöÄ\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "MEU_RU = \"3666900\"\n",
        "appName = f\"PySpark Trabalho de Big Data - {MEU_RU}\"\n",
        "master = \"local\"\n",
        "\n",
        "print(f\"Iniciando spark session para Bruno - RU {MEU_RU}\")\n",
        "spark: SparkSession = SparkSession.builder.appName(appName).master(master).getOrCreate()\n",
        "print(\"Sess√£o iniciado com sucesso! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z64GGf0X_WDV"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o m√©todo read.csv do spark\n",
        "\n",
        "N√£o altere este c√≥digo e use o dataframe imdb_df criado aqui em todo o seu trabalho. A cria√ß√£o de um dataframe diferente deste poder√° causar erros na coluna sentiment e isso refletir√° em erros de resposta das quest√µes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BZB-9I1N_WDV"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3xUmmAh_WDW"
      },
      "source": [
        "# Quest√£o 1\n",
        "\n",
        "Nesta quest√£o, voc√™ ir√° calcular a soma dos IDs para entradas onde o sentimento ('sentiment') √© 'neg'.\n",
        "\n",
        "### Objetivo:\n",
        "- Usar a coluna 'sentiment' como chave e somar os valores da coluna 'id'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh1I82TL_WDW"
      },
      "source": [
        "## Criar fun√ß√µes de MAP:\n",
        "- Criar fun√ß√£o para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro\n",
        "\n",
        "A fun√ß√£o map ir√° transformar cada linha do dataframe em uma **tupla** (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: coluna 'id' convertida para inteiro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "LBwO8fp-_WDW"
      },
      "outputs": [],
      "source": [
        "# Quest√£o 1\n",
        "def map1(x):\n",
        "    MEU_RU = '3666900'\n",
        "    print(f\"Meu RU √© {MEU_RU}\")\n",
        "    if x['sentiment'] is None or x['id'] is None:\n",
        "        return None  # N√£o retorna nada se algum valor faltar\n",
        "    return (x['sentiment'], int(x['id']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK5k7s_h_WDW"
      },
      "source": [
        "## Cria fun√ß√µes de REDUCE:\n",
        "\n",
        "- Criar fun√ß√£o de reduce para somar os IDs por \"sentiment\".\n",
        "\n",
        "A fun√ß√£o reduce ir√° somar os valores dos IDs agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "D_VIL49O_WDW"
      },
      "outputs": [],
      "source": [
        "def reduceByKey1(x, y):\n",
        "    MEU_RU = '3666900'\n",
        "    print(f\"Meu RU √© {MEU_RU}\")\n",
        "    # A fun√ß√£o vai somar os valores de id\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMQnR0Ku_WDX"
      },
      "source": [
        "## Aplica√ß√£o do map/reduce e visualiza√ß√£o do resultado\n",
        "\n",
        "Aqui, voc√™ aplicar√° as fun√ß√µes de map e reduce ao dataframe Spark para calcular os resultados. N√£o se esque√ßa de usar o m√©todo `.collect()` para visualizar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "collapsed": true,
        "id": "aoJTAqhp_WDX",
        "outputId": "97d47534-3965-4013-eebc-f453e0a9419b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado da Quest√£o 1:\n",
            "Sentimento: neg, Soma dos IDs: 459568555\n",
            "Sentimento: pos, Soma dos IDs: 763600041\n",
            "------------------------------\n",
            "Meu nome √© Bruno Costa Sampaio\n",
            "Meu RU √© 3666900\n"
          ]
        }
      ],
      "source": [
        "# Aplica√ß√£o do map/reduce para Quest√£o 1\n",
        "resultado_questao1 = imdb_df.rdd.map(map1).filter(lambda x: x is not None).reduceByKey(reduceByKey1).collect()\n",
        "\n",
        "# Exibir o resultado da Quest√£o 1\n",
        "print(\"Resultado da Quest√£o 1:\")\n",
        "for item in resultado_questao1:\n",
        "    print(f\"Sentimento: {item[0]}, Soma dos IDs: {item[1]}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "nome = \"Bruno Costa Sampaio\"\n",
        "ru = \"3666900\"\n",
        "print(f\"Meu nome √© {nome}\")\n",
        "print(f\"Meu RU √© {ru}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADv-dyEU_WDX"
      },
      "source": [
        "# Quest√£o 2:\n",
        "\n",
        "Nesta quest√£o, voc√™ ir√° calcular a diferen√ßa no n√∫mero total de palavras entre textos negativos em portugu√™s e ingl√™s.\n",
        "\n",
        "### Objetivo:\n",
        "- Contar as palavras em cada idioma (colunas 'text_pt' e 'text_en') para entradas onde o sentimento ('sentiment') √© 'neg'.\n",
        "- Subtrair o total de palavras em ingl√™s do total em portugu√™s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gun2xRly_WDX"
      },
      "source": [
        "## Criar fun√ß√µes de MAP:\n",
        "- Criar fun√ß√£o para mapear o \"sentiment\" como chave de uma tupla principal e como valor uma outra tupla com a soma das palavras de cada idioma como valor.\n",
        "\n",
        "A fun√ß√£o map ir√° transformar cada linha do dataframe em uma tupla (chave-valor), onde:\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: Nova tupla com:\n",
        "  - Elemento 0: soma das palavras da coluna 'text_en'\n",
        "  - Elemento 1: soma das palavras da coluna 'text_pt'\n",
        "\n",
        "OU\n",
        "- Chave: coluna 'sentiment'\n",
        "- Valor: (soma das palavras da coluna 'text_pt') - (soma das palavras da coluna 'text_en')\n",
        "  \n",
        "\n",
        "Para contar as palavras deve-se primeiro separar os textos em uma lista de palavras para ent√£o descobrir o tamanho desta lista.\n",
        "Dicas:\n",
        "\n",
        "1. Use o m√©todo .split() e n√£o .split(\" \") de string para separar as palavras em uma lista ou use a fun√ß√£o split(coluna de texto, regex) do pyspark com o regex igual √† \"[ ]+\" ou \"\\s+\"\n",
        "2. Use len() para descobrir o tamanho da lista de palavras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "42O7w7EF_WDX"
      },
      "outputs": [],
      "source": [
        "# Quest√£o 2\n",
        "def map2_filtered(x):\n",
        "    if x['sentiment'] == 'neg':  # S√≥ processar sentimentos negativos\n",
        "        words_en = len(x['text_en'].split()) if x['text_en'] else 0\n",
        "        words_pt = len(x['text_pt'].split()) if x['text_pt'] else 0\n",
        "        return (x['sentiment'], (words_en, words_pt))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWT4FFQA_WDX"
      },
      "source": [
        "## Cria fun√ß√µes de REDUCE:\n",
        "\n",
        "- Criar fun√ß√£o de reduce para somar o numero de palavras de cada texto portugu√™s e ingl√™s por \"sentiment\" (depender√° de como voc√™ optou por fazer sua fun√ß√£o map2).\n",
        "\n",
        "A fun√ß√£o reduce ir√° somar os valores das quantidades de palavras agrupados por chave ('sentiment')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "P4Fd3-LP_WDX"
      },
      "outputs": [],
      "source": [
        "def reduceByKey2(x, y):\n",
        "    # x e y s√£o tuplas no formato (contagem_en, contagem_pt)\n",
        "    total_words_en = x[0] + y[0]\n",
        "    total_words_pt = x[1] + y[1]\n",
        "    return (total_words_en, total_words_pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWKbYx23_WDY"
      },
      "source": [
        "## Aplica√ß√£o do map/reduce e visualiza√ß√£o do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtra√ß√£o.\n",
        "3. Realizar a subtra√ß√£o das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "tQt0LL2-_WDY",
        "outputId": "fd70adb4-ac32-4fbc-aba9-924af14540ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado da Quest√£o 2:\n",
            "Meu nome √© Bruno Costa Sampaio\n",
            "Meu RU √© 3666900\n"
          ]
        }
      ],
      "source": [
        "# Aplica√ß√£o do map/reduce para Quest√£o 2\n",
        "resultado_questao2 = imdb_df.rdd.map(map2_filtered).filter(lambda x: x is not None).reduceByKey(reduceByKey2).collect()\n",
        "\n",
        "# Exibir o resultado da Quest√£o 2\n",
        "print(\"Resultado da Quest√£o 2:\")\n",
        "for sentimento, (total_en, total_pt) in resultado_questao2:\n",
        "    if sentimento == 'neg':\n",
        "        diferenca_palavras = total_pt - total_en\n",
        "        break\n",
        "\n",
        "nome = \"Bruno Costa Sampaio\"\n",
        "ru = \"3666900\"\n",
        "print(f\"Meu nome √© {nome}\")\n",
        "print(f\"Meu RU √© {ru}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}